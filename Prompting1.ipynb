{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T14:59:02.493347Z",
     "iopub.status.busy": "2024-11-12T14:59:02.492879Z",
     "iopub.status.idle": "2024-11-12T14:59:37.579587Z",
     "shell.execute_reply": "2024-11-12T14:59:37.578318Z",
     "shell.execute_reply.started": "2024-11-12T14:59:02.493301Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -U -q \"google-generativeai>=0.8.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:00:24.945617Z",
     "iopub.status.busy": "2024-11-12T15:00:24.945142Z",
     "iopub.status.idle": "2024-11-12T15:00:26.311650Z",
     "shell.execute_reply": "2024-11-12T15:00:26.310526Z",
     "shell.execute_reply.started": "2024-11-12T15:00:24.945569Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "from IPython.display import HTML, Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:06:05.915011Z",
     "iopub.status.busy": "2024-11-12T15:06:05.914323Z",
     "iopub.status.idle": "2024-11-12T15:06:06.106105Z",
     "shell.execute_reply": "2024-11-12T15:06:06.105083Z",
     "shell.execute_reply.started": "2024-11-12T15:06:05.914950Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "GOOGLE_API_KEY = UserSecretsClient().get_secret(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=\"AIzaSyCzZWKV4KNLNQmUYh2K98YkoHRzY22lErc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:09:44.780349Z",
     "iopub.status.busy": "2024-11-12T15:09:44.779830Z",
     "iopub.status.idle": "2024-11-12T15:09:46.133221Z",
     "shell.execute_reply": "2024-11-12T15:09:46.131916Z",
     "shell.execute_reply.started": "2024-11-12T15:09:44.780302Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imagine you have a really smart friend who loves learning new things. This friend can look at lots of pictures of cats and dogs and learn to tell the difference between them. Then, if you show them a new picture, they can tell you if it's a cat or a dog!\n",
      "\n",
      "That's kind of what AI is like. It's like a super smart computer that can learn from lots of data and then use that knowledge to do things. For example, AI can help your phone understand what you're saying when you talk to it, or it can help a car drive itself!\n",
      "\n",
      "AI is still learning and growing, just like your friend is learning more every day. But it can already do some amazing things! \n",
      "\n"
     ]
    }
   ],
   "source": [
    "flash = genai.GenerativeModel('gemini-1.5-flash')\n",
    "response = flash.generate_content(\"Explain AI to me like I'm a kid.\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:10:39.905585Z",
     "iopub.status.busy": "2024-11-12T15:10:39.904269Z",
     "iopub.status.idle": "2024-11-12T15:10:39.915309Z",
     "shell.execute_reply": "2024-11-12T15:10:39.913909Z",
     "shell.execute_reply.started": "2024-11-12T15:10:39.905535Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Imagine you have a really smart friend who loves learning new things. This friend can look at lots of pictures of cats and dogs and learn to tell the difference between them. Then, if you show them a new picture, they can tell you if it's a cat or a dog!\n",
       "\n",
       "That's kind of what AI is like. It's like a super smart computer that can learn from lots of data and then use that knowledge to do things. For example, AI can help your phone understand what you're saying when you talk to it, or it can help a car drive itself!\n",
       "\n",
       "AI is still learning and growing, just like your friend is learning more every day. But it can already do some amazing things! \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:11:40.295750Z",
     "iopub.status.busy": "2024-11-12T15:11:40.295307Z",
     "iopub.status.idle": "2024-11-12T15:11:40.766470Z",
     "shell.execute_reply": "2024-11-12T15:11:40.765187Z",
     "shell.execute_reply.started": "2024-11-12T15:11:40.295707Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Ratchamarri! ðŸ‘‹  It's nice to meet you. ðŸ˜„  What can I do for you today? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat = flash.start_chat(history=[])\n",
    "response = chat.send_message('Hello! My name is ratchamarri.')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:11:59.920118Z",
     "iopub.status.busy": "2024-11-12T15:11:59.919060Z",
     "iopub.status.idle": "2024-11-12T15:12:01.723886Z",
     "shell.execute_reply": "2024-11-12T15:12:01.722703Z",
     "shell.execute_reply.started": "2024-11-12T15:11:59.920068Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay, here's something interesting about dinosaurs:\n",
      "\n",
      "**Did you know that some dinosaurs had feathers?**\n",
      "\n",
      "While the image of scaly, reptilian dinosaurs is iconic, it's not the full story. Many dinosaurs, including some of the most famous like Velociraptor and Tyrannosaurus Rex, actually had feathers. \n",
      "\n",
      "* **Evidence:** Scientists have found fossilized feathers on a wide range of dinosaurs, offering a glimpse into their appearance and behavior. These feathers ranged from simple, downy filaments to more complex, flight-capable structures.\n",
      "\n",
      "* **Evolution:**  The presence of feathers suggests that dinosaurs were more closely related to birds than previously thought. It's believed that feathers evolved for insulation, display, and eventually flight. \n",
      "\n",
      "* **Redefining Dinosaurs:** The discovery of feathered dinosaurs has revolutionized our understanding of these ancient creatures and their relationship to birds.\n",
      "\n",
      "**This means that the next time you picture a dinosaur, you might want to imagine it sporting some feathers!** \n",
      "\n",
      "Do you want to learn more about dinosaurs or maybe a specific kind? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = chat.send_message('Can you tell something interesting about dinosaurs?')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:12:28.462000Z",
     "iopub.status.busy": "2024-11-12T15:12:28.461528Z",
     "iopub.status.idle": "2024-11-12T15:12:29.137914Z",
     "shell.execute_reply": "2024-11-12T15:12:29.136732Z",
     "shell.execute_reply.started": "2024-11-12T15:12:28.461941Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You bet!  Your name is Ratchamarri. ðŸ˜Š  I'm still getting to know you, but I'm trying my best to remember. ðŸ˜„ \n",
      "\n",
      "Is there anything else you'd like to talk about? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# While you have the `chat` object around, the conversation state\n",
    "# persists. Confirm that by asking if it knows my name.\n",
    "response = chat.send_message('Do you remember what my name is?')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:12:52.861723Z",
     "iopub.status.busy": "2024-11-12T15:12:52.861267Z",
     "iopub.status.idle": "2024-11-12T15:12:53.059220Z",
     "shell.execute_reply": "2024-11-12T15:12:53.058037Z",
     "shell.execute_reply.started": "2024-11-12T15:12:52.861680Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "models/chat-bison-001\n",
      "models/text-bison-001\n",
      "models/embedding-gecko-001\n",
      "models/gemini-1.0-pro-latest\n",
      "models/gemini-1.0-pro\n",
      "models/gemini-pro\n",
      "models/gemini-1.0-pro-001\n",
      "models/gemini-1.0-pro-vision-latest\n",
      "models/gemini-pro-vision\n",
      "models/gemini-1.5-pro-latest\n",
      "models/gemini-1.5-pro-001\n",
      "models/gemini-1.5-pro-002\n",
      "models/gemini-1.5-pro\n",
      "models/gemini-1.5-pro-exp-0801\n",
      "models/gemini-1.5-pro-exp-0827\n",
      "models/gemini-1.5-flash-latest\n",
      "models/gemini-1.5-flash-001\n",
      "models/gemini-1.5-flash-001-tuning\n",
      "models/gemini-1.5-flash\n",
      "models/gemini-1.5-flash-exp-0827\n",
      "models/gemini-1.5-flash-002\n",
      "models/gemini-1.5-flash-8b\n",
      "models/gemini-1.5-flash-8b-001\n",
      "models/gemini-1.5-flash-8b-latest\n",
      "models/gemini-1.5-flash-8b-exp-0827\n",
      "models/gemini-1.5-flash-8b-exp-0924\n",
      "models/embedding-001\n",
      "models/text-embedding-004\n",
      "models/aqa\n"
     ]
    }
   ],
   "source": [
    "for model in genai.list_models():\n",
    "  print(model.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:13:09.045579Z",
     "iopub.status.busy": "2024-11-12T15:13:09.045145Z",
     "iopub.status.idle": "2024-11-12T15:13:09.218201Z",
     "shell.execute_reply": "2024-11-12T15:13:09.217021Z",
     "shell.execute_reply.started": "2024-11-12T15:13:09.045536Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(name='models/gemini-1.5-flash',\n",
      "      base_model_id='',\n",
      "      version='001',\n",
      "      display_name='Gemini 1.5 Flash',\n",
      "      description='Fast and versatile multimodal model for scaling across diverse tasks',\n",
      "      input_token_limit=1000000,\n",
      "      output_token_limit=8192,\n",
      "      supported_generation_methods=['generateContent', 'countTokens'],\n",
      "      temperature=1.0,\n",
      "      max_temperature=2.0,\n",
      "      top_p=0.95,\n",
      "      top_k=40)\n"
     ]
    }
   ],
   "source": [
    "for model in genai.list_models():\n",
    "  if model.name == 'models/gemini-1.5-flash':\n",
    "    print(model)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:13:29.512616Z",
     "iopub.status.busy": "2024-11-12T15:13:29.511696Z",
     "iopub.status.idle": "2024-11-12T15:13:31.200075Z",
     "shell.execute_reply": "2024-11-12T15:13:31.198890Z",
     "shell.execute_reply.started": "2024-11-12T15:13:29.512563Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## The Olive: A Timeless Treasure in Modern Society\n",
      "\n",
      "The olive, a humble fruit with a rich history spanning millennia, continues to hold a significant place in modern society. Its cultural significance, nutritional value, and versatile applications across diverse industries make it a vital component of global food systems, economies, and even cultural identity. This essay explores the multifaceted importance of olives in the 21st century, highlighting their role in gastronomy, health, environmental sustainability, and cultural heritage.\n",
      "\n",
      "**A Culinary Icon and Flavorful Staple:**\n",
      "\n",
      "Olives are a culinary icon, revered for their unique flavor profile and versatility in the kitchen. From the bright green, briny flavor of Kalamata olives to the subtle, buttery taste of Manzanillo olives, their diverse varieties offer a range of tastes and textures.  In the Mediterranean region, olives are a staple ingredient, enjoyed as a snack, a condiment, or a key component in numerous traditional dishes. \n",
      "\n",
      "The olive's culinary significance extends beyond\n"
     ]
    }
   ],
   "source": [
    "short_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(max_output_tokens=200))\n",
    "\n",
    "response = short_model.generate_content('Write a 1000 word essay on the importance of olives in modern society.')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:13:51.600434Z",
     "iopub.status.busy": "2024-11-12T15:13:51.599981Z",
     "iopub.status.idle": "2024-11-12T15:13:52.743862Z",
     "shell.execute_reply": "2024-11-12T15:13:52.742747Z",
     "shell.execute_reply.started": "2024-11-12T15:13:51.600386Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A tiny fruit, with skin of green or black,\n",
      "A humble olive, on a twisted track,\n",
      "From sun-drenched groves, a journey to our plate,\n",
      "A taste of history, a culinary fate.\n",
      "\n",
      "In ancient lands, a symbol of peace and grace,\n",
      "Now oil and brine, their flavors we embrace.\n",
      "On pizzas, salads, bread, a savory delight,\n",
      "A versatile ingredient, day and night.\n",
      "\n",
      "From simple snacks, to gourmet feasts refined,\n",
      "The olive's presence, in our lives entwined.\n",
      "A source of health, a taste that we adore,\n",
      "The humble olive, forevermore. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "response = short_model.generate_content('Write a short poem on the importance of olives in modern society.')\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:17:28.296147Z",
     "iopub.status.busy": "2024-11-12T15:17:28.295634Z",
     "iopub.status.idle": "2024-11-12T15:17:30.188326Z",
     "shell.execute_reply": "2024-11-12T15:17:30.187014Z",
     "shell.execute_reply.started": "2024-11-12T15:17:28.296101Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purple \n",
      " -------------------------\n",
      "Blue \n",
      " -------------------------\n",
      "Teal \n",
      " -------------------------\n",
      "Indigo \n",
      " -------------------------\n",
      "Purple \n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "from google.api_core import retry\n",
    "\n",
    "high_temp_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(temperature=1.5))\n",
    "\n",
    "\n",
    "# When running lots of queries, it's a good practice to use a retry policy so your code\n",
    "# automatically retries when hitting Resource Exhausted (quota limit) errors.\n",
    "retry_policy = {\n",
    "    \"retry\": retry.Retry(predicate=retry.if_transient_error, initial=10, multiplier=1.5, timeout=300)\n",
    "}\n",
    "\n",
    "for _ in range(5):\n",
    "  response = high_temp_model.generate_content('Pick a random colour... (respond in a single word)',\n",
    "                                              request_options=retry_policy)\n",
    "  if response.parts:\n",
    "    print(response.text, '-' * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:16:28.102676Z",
     "iopub.status.busy": "2024-11-12T15:16:28.101608Z",
     "iopub.status.idle": "2024-11-12T15:16:29.848839Z",
     "shell.execute_reply": "2024-11-12T15:16:29.847643Z",
     "shell.execute_reply.started": "2024-11-12T15:16:28.102630Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blue \n",
      " -------------------------\n",
      "Purple. \n",
      " -------------------------\n",
      "Purple \n",
      " -------------------------\n",
      "Purple \n",
      " -------------------------\n",
      "Blue \n",
      " -------------------------\n"
     ]
    }
   ],
   "source": [
    "low_temp_model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash',\n",
    "    generation_config=genai.GenerationConfig(temperature=2.0))\n",
    "\n",
    "for _ in range(5):\n",
    "  response = low_temp_model.generate_content('Pick a random colour... (respond in a single word)',\n",
    "                                             request_options=retry_policy)\n",
    "  if response.parts:\n",
    "    print(response.text, '-' * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:18:20.346932Z",
     "iopub.status.busy": "2024-11-12T15:18:20.346503Z",
     "iopub.status.idle": "2024-11-12T15:18:23.169227Z",
     "shell.execute_reply": "2024-11-12T15:18:23.168029Z",
     "shell.execute_reply.started": "2024-11-12T15:18:20.346892Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bartholomew the tabby wasn't your average house cat. He yearned for adventure, for the unknown, for a world beyond the familiar scent of tuna and the warm sunbeams on the living room rug. His owner, Mrs. Peabody, a kindly but rather oblivious woman, kept him well-fed and cuddled, but she never once suspected the secret life Bartholomew lived. \n",
      "\n",
      "One day, Mrs. Peabody left the back door ajar, and the scent of freedom wafted in. Bartholomew, seized by a sudden surge of excitement, slipped out into the unknown. The world was a symphony of sights and sounds - the rustle of leaves, the chirping of birds, the scent of damp earth. He stalked a fat robin in the backyard, chased a butterfly across the lawn, and even attempted to climb a tree (with disastrous, but ultimately hilarious, results).\n",
      "\n",
      "His adventures led him to a bustling park, where he met a ginger cat named Jasper, who was notorious for his daring escapades. Jasper, recognizing Bartholomew's adventurous spirit, took him under his wing, showing him the ropes of life as a street cat. \n",
      "\n",
      "Together, they embarked on a series of escapades: they raided a picnic basket, snuck into a bakery for a taste of warm bread, and even outsmarted a grumpy old dog guarding a juicy bone.  Bartholomew, emboldened by his new friend, discovered a thrill in the chase, a sense of satisfaction in his victories, and a love for the unpredictable nature of street life.\n",
      "\n",
      "But as days turned into weeks, a pang of longing grew in Bartholomew's heart. He missed Mrs. Peabody's soft strokes and the comforting routine of his home. One evening, as the moon cast long shadows, he found himself back at Mrs. Peabody's doorstep. \n",
      "\n",
      "He crept inside, his fur dusted with the grime of the streets, a mischievous glint in his eyes. Mrs. Peabody, startled by his presence, gasped, \"Bartholomew! Where have you been?\"\n",
      "\n",
      "Bartholomew, curled up on his favorite spot on the rug, purred contentedly, his tail twitching with a secret knowledge. He had tasted freedom, but home was where his heart truly belonged.  He knew he wouldn't be a typical house cat, but he would cherish the quiet moments of domestic bliss, knowing that the thrill of adventure was always just a cracked window away.  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-001',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        # These are the default values for gemini-1.5-flash-001.\n",
    "        temperature=1.0,\n",
    "        top_k=64,\n",
    "        top_p=0.95,\n",
    "    ))\n",
    "\n",
    "story_prompt = \"You are a creative writer. Write a short story about a cat who goes on an adventure.\"\n",
    "response = model.generate_content(story_prompt, request_options=retry_policy)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:19:06.671057Z",
     "iopub.status.busy": "2024-11-12T15:19:06.669743Z",
     "iopub.status.idle": "2024-11-12T15:19:07.037590Z",
     "shell.execute_reply": "2024-11-12T15:19:07.036372Z",
     "shell.execute_reply.started": "2024-11-12T15:19:06.670992Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: **POSITIVE**\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-001',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=5,\n",
    "    ))\n",
    "\n",
    "zero_shot_prompt = \"\"\"Classify movie reviews as POSITIVE, NEUTRAL or NEGATIVE.\n",
    "Review: \"Her\" is a disturbing study revealing the direction\n",
    "humanity is headed if AI is allowed to keep evolving,\n",
    "unchecked. I wish there were more movies like this masterpiece.\n",
    "Sentiment: \"\"\"\n",
    "\n",
    "response = model.generate_content(zero_shot_prompt, request_options=retry_policy)\n",
    "print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:19:43.978658Z",
     "iopub.status.busy": "2024-11-12T15:19:43.978225Z",
     "iopub.status.idle": "2024-11-12T15:19:44.581759Z",
     "shell.execute_reply": "2024-11-12T15:19:44.580464Z",
     "shell.execute_reply.started": "2024-11-12T15:19:43.978614Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "positive\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "\n",
    "class Sentiment(enum.Enum):\n",
    "    POSITIVE = \"positive\"\n",
    "    NEUTRAL = \"neutral\"\n",
    "    NEGATIVE = \"negative\"\n",
    "\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-001',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        response_mime_type=\"text/x.enum\",\n",
    "        response_schema=Sentiment\n",
    "    ))\n",
    "\n",
    "response = model.generate_content(zero_shot_prompt, request_options=retry_policy)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:20:12.573475Z",
     "iopub.status.busy": "2024-11-12T15:20:12.573021Z",
     "iopub.status.idle": "2024-11-12T15:20:13.020543Z",
     "shell.execute_reply": "2024-11-12T15:20:13.019332Z",
     "shell.execute_reply.started": "2024-11-12T15:20:12.573432Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "\"size\": \"large\",\n",
      "\"type\": \"normal\",\n",
      "\"ingredients\": [\"cheese\", \"pineapple\"]\n",
      "}\n",
      "``` \n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=250,\n",
    "    ))\n",
    "\n",
    "few_shot_prompt = \"\"\"Parse a customer's pizza order into valid JSON:\n",
    "\n",
    "EXAMPLE:\n",
    "I want a small pizza with cheese, tomato sauce, and pepperoni.\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"small\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"cheese\", \"tomato sauce\", \"peperoni\"]\n",
    "}\n",
    "```\n",
    "\n",
    "EXAMPLE:\n",
    "Can I get a large pizza with tomato sauce, basil and mozzarella\n",
    "JSON Response:\n",
    "```\n",
    "{\n",
    "\"size\": \"large\",\n",
    "\"type\": \"normal\",\n",
    "\"ingredients\": [\"tomato sauce\", \"basil\", \"mozzarella\"]\n",
    "}\n",
    "\n",
    "ORDER:\n",
    "\"\"\"\n",
    "\n",
    "customer_order = \"Give me a large with cheese & pineapple\"\n",
    "\n",
    "\n",
    "response = model.generate_content([few_shot_prompt, customer_order], request_options=retry_policy)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:20:40.070108Z",
     "iopub.status.busy": "2024-11-12T15:20:40.069531Z",
     "iopub.status.idle": "2024-11-12T15:20:40.509240Z",
     "shell.execute_reply": "2024-11-12T15:20:40.507871Z",
     "shell.execute_reply.started": "2024-11-12T15:20:40.070064Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"ingredients\": [\"apple\", \"chocolate\"], \"size\": \"large\", \"type\": \"dessert\"}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import typing_extensions as typing\n",
    "\n",
    "class PizzaOrder(typing.TypedDict):\n",
    "    size: str\n",
    "    ingredients: list[str]\n",
    "    type: str\n",
    "\n",
    "\n",
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=0.1,\n",
    "        response_mime_type=\"application/json\",\n",
    "        response_schema=PizzaOrder,\n",
    "    ))\n",
    "\n",
    "response = model.generate_content(\"Can I have a large dessert pizza with apple and chocolate\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:21:06.343812Z",
     "iopub.status.busy": "2024-11-12T15:21:06.342935Z",
     "iopub.status.idle": "2024-11-12T15:21:06.893309Z",
     "shell.execute_reply": "2024-11-12T15:21:06.892041Z",
     "shell.execute_reply.started": "2024-11-12T15:21:06.343746Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "When you were 4, your partner was 3 * 4 = 12 years old.\n",
      "\n",
      "The age difference between you is 12 - 4 = 8 years.\n",
      "\n",
      "Therefore, your partner is now 20 + 8 = **28 years old**. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now, I\n",
    "am 20 years old. How old is my partner? Return the answer immediately.\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "response = model.generate_content(prompt, request_options=retry_policy)\n",
    "\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:21:29.653233Z",
     "iopub.status.busy": "2024-11-12T15:21:29.652774Z",
     "iopub.status.idle": "2024-11-12T15:21:30.300578Z",
     "shell.execute_reply": "2024-11-12T15:21:30.299389Z",
     "shell.execute_reply.started": "2024-11-12T15:21:29.653184Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's how to solve this:\n",
      "\n",
      "* **When you were 4:** Your partner was 3 times your age, which is 4 * 3 = 12 years old.\n",
      "* **Age difference:**  Your partner is 12 - 4 = 8 years older than you.\n",
      "* **Current age:** Since you're now 20, your partner is 20 + 8 = **28 years old**. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"When I was 4 years old, my partner was 3 times my age. Now,\n",
    "I am 20 years old. How old is my partner? Let's think step by step.\"\"\"\n",
    "\n",
    "response = model.generate_content(prompt, request_options=retry_policy)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:22:10.285413Z",
     "iopub.status.busy": "2024-11-12T15:22:10.284341Z",
     "iopub.status.idle": "2024-11-12T15:22:10.294583Z",
     "shell.execute_reply": "2024-11-12T15:22:10.293181Z",
     "shell.execute_reply.started": "2024-11-12T15:22:10.285359Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model_instructions = \"\"\"\n",
    "Solve a question answering task with interleaving Thought, Action, Observation steps. Thought can reason about the current situation,\n",
    "Observation is understanding relevant information from an Action's output and Action can be one of three types:\n",
    " (1) <search>entity</search>, which searches the exact entity on Wikipedia and returns the first paragraph if it exists. If not, it\n",
    "     will return some similar entities to search and you can try to search the information from those topics.\n",
    " (2) <lookup>keyword</lookup>, which returns the next sentence containing keyword in the current context. This only does exact matches,\n",
    "     so keep your searches short.\n",
    " (3) <finish>answer</finish>, which returns the answer and finishes the task.\n",
    "\"\"\"\n",
    "\n",
    "example1 = \"\"\"Question\n",
    "Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\n",
    "\n",
    "Thought 1\n",
    "The question simplifies to \"The Simpsons\" character Milhouse is named after who. I only need to search Milhouse and find who it is named after.\n",
    "\n",
    "Action 1\n",
    "<search>Milhouse</search>\n",
    "\n",
    "Observation 1\n",
    "Milhouse Mussolini Van Houten is a recurring character in the Fox animated television series The Simpsons voiced by Pamela Hayden and created by Matt Groening.\n",
    "\n",
    "Thought 2\n",
    "The paragraph does not tell who Milhouse is named after, maybe I can look up \"named after\".\n",
    "\n",
    "Action 2\n",
    "<lookup>named after</lookup>\n",
    "\n",
    "Observation 2\n",
    "Milhouse was named after U.S. president Richard Nixon, whose middle name was Milhous.\n",
    "\n",
    "Thought 3\n",
    "Milhouse was named after U.S. president Richard Nixon, so the answer is Richard Nixon.\n",
    "\n",
    "Action 3\n",
    "<finish>Richard Nixon</finish>\n",
    "\"\"\"\n",
    "\n",
    "example2 = \"\"\"Question\n",
    "What is the elevation range for the area that the eastern sector of the Colorado orogeny extends into?\n",
    "\n",
    "Thought 1\n",
    "I need to search Colorado orogeny, find the area that the eastern sector of the Colorado orogeny extends into, then find the elevation range of the area.\n",
    "\n",
    "Action 1\n",
    "<search>Colorado orogeny</search>\n",
    "\n",
    "Observation 1\n",
    "The Colorado orogeny was an episode of mountain building (an orogeny) in Colorado and surrounding areas.\n",
    "\n",
    "Thought 2\n",
    "It does not mention the eastern sector. So I need to look up eastern sector.\n",
    "\n",
    "Action 2\n",
    "<lookup>eastern sector</lookup>\n",
    "\n",
    "Observation 2\n",
    "The eastern sector extends into the High Plains and is called the Central Plains orogeny.\n",
    "\n",
    "Thought 3\n",
    "The eastern sector of Colorado orogeny extends into the High Plains. So I need to search High Plains and find its elevation range.\n",
    "\n",
    "Action 3\n",
    "<search>High Plains</search>\n",
    "\n",
    "Observation 3\n",
    "High Plains refers to one of two distinct land regions\n",
    "\n",
    "Thought 4\n",
    "I need to instead search High Plains (United States).\n",
    "\n",
    "Action 4\n",
    "<search>High Plains (United States)</search>\n",
    "\n",
    "Observation 4\n",
    "The High Plains are a subregion of the Great Plains. From east to west, the High Plains rise in elevation from around 1,800 to 7,000 ft (550 to 2,130m).\n",
    "\n",
    "Thought 5\n",
    "High Plains rise in elevation from around 1,800 to 7,000 ft, so the answer is 1,800 to 7,000 ft.\n",
    "\n",
    "Action 5\n",
    "<finish>1,800 to 7,000 ft</finish>\n",
    "\"\"\"\n",
    "\n",
    "# Come up with more examples yourself, or take a look through https://github.com/ysymyth/ReAct/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:22:26.303076Z",
     "iopub.status.busy": "2024-11-12T15:22:26.302601Z",
     "iopub.status.idle": "2024-11-12T15:22:28.853926Z",
     "shell.execute_reply": "2024-11-12T15:22:28.852627Z",
     "shell.execute_reply.started": "2024-11-12T15:22:26.303030Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 1\n",
      "I need to search for the transformers NLP paper and find the list of authors. Then I need to find the youngest author.\n",
      "\n",
      "Action 1\n",
      "<search>transformers NLP paper</search>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"\"\"Question\n",
    "Who was the youngest author listed on the transformers NLP paper?\n",
    "\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "react_chat = model.start_chat()\n",
    "\n",
    "# You will perform the Action, so generate up to, but not including, the Observation.\n",
    "config = genai.GenerationConfig(stop_sequences=[\"\\nObservation\"])\n",
    "\n",
    "resp = react_chat.send_message(\n",
    "    [model_instructions, example1, example2, question],\n",
    "    generation_config=config,\n",
    "    request_options=retry_policy)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:23:02.922832Z",
     "iopub.status.busy": "2024-11-12T15:23:02.922308Z",
     "iopub.status.idle": "2024-11-12T15:23:03.548526Z",
     "shell.execute_reply": "2024-11-12T15:23:03.547055Z",
     "shell.execute_reply.started": "2024-11-12T15:23:02.922736Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thought 2\n",
      "The Observation gives me a list of authors. The question asks for the youngest author but doesn't specify how to determine age. I'll need to assume a way to guess the youngest author.\n",
      "\n",
      "Action 2\n",
      "<finish>Aidan N. Gomez</finish> \n",
      "\n"
     ]
    }
   ],
   "source": [
    "observation = \"\"\"Observation 1\n",
    "[1706.03762] Attention Is All You Need\n",
    "Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, Illia Polosukhin\n",
    "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
    "\"\"\"\n",
    "resp = react_chat.send_message(observation, generation_config=config, request_options=retry_policy)\n",
    "print(resp.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:23:26.304616Z",
     "iopub.status.busy": "2024-11-12T15:23:26.304189Z",
     "iopub.status.idle": "2024-11-12T15:23:26.751310Z",
     "shell.execute_reply": "2024-11-12T15:23:26.750249Z",
     "shell.execute_reply.started": "2024-11-12T15:23:26.304574Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def factorial(n):\n",
       "  if n == 0:\n",
       "    return 1\n",
       "  else:\n",
       "    return n * factorial(n-1)\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    generation_config=genai.GenerationConfig(\n",
    "        temperature=1,\n",
    "        top_p=1,\n",
    "        max_output_tokens=1024,\n",
    "    ))\n",
    "\n",
    "# Gemini 1.5 models are very chatty, so it helps to specify they stick to the code.\n",
    "code_prompt = \"\"\"\n",
    "Write a Python function to calculate the factorial of a number. No explanation, provide only the code.\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(code_prompt, request_options=retry_policy)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:23:46.280813Z",
     "iopub.status.busy": "2024-11-12T15:23:46.280330Z",
     "iopub.status.idle": "2024-11-12T15:23:51.009707Z",
     "shell.execute_reply": "2024-11-12T15:23:51.008530Z",
     "shell.execute_reply.started": "2024-11-12T15:23:46.280767Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I will use Python to calculate the sum of the first 14 odd prime numbers.\n",
       "\n",
       "\n",
       "``` python\n",
       "import sympy\n",
       "\n",
       "primes = [x for x in sympy.primerange(1, 100) if x % 2 != 0]\n",
       "print(f'{primes[:14]=}')\n",
       "print(f'sum(primes[:14]) = {sum(primes[:14])}')\n",
       "\n",
       "```\n",
       "```\n",
       "primes[:14]=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\n",
       "sum(primes[:14]) = 326\n",
       "\n",
       "```\n",
       "The sum of the first 14 odd prime numbers is 326. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = genai.GenerativeModel(\n",
    "    'gemini-1.5-flash-latest',\n",
    "    tools='code_execution',)\n",
    "\n",
    "code_exec_prompt = \"\"\"\n",
    "Calculate the sum of the first 14 prime numbers. Only consider the odd primes, and make sure you count them all.\n",
    "\"\"\"\n",
    "\n",
    "response = model.generate_content(code_exec_prompt, request_options=retry_policy)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:24:18.312833Z",
     "iopub.status.busy": "2024-11-12T15:24:18.311788Z",
     "iopub.status.idle": "2024-11-12T15:24:18.801900Z",
     "shell.execute_reply": "2024-11-12T15:24:18.800670Z",
     "shell.execute_reply.started": "2024-11-12T15:24:18.312781Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum_of_primes=326\n"
     ]
    }
   ],
   "source": [
    "import sympy\n",
    "\n",
    "primes = sympy.primerange(0, 100)\n",
    "odd_primes = [p for p in primes if p % 2 != 0]\n",
    "sum_of_primes = sum(odd_primes[:14])\n",
    "print(f'{sum_of_primes=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:24:36.459419Z",
     "iopub.status.busy": "2024-11-12T15:24:36.458720Z",
     "iopub.status.idle": "2024-11-12T15:24:36.466291Z",
     "shell.execute_reply": "2024-11-12T15:24:36.465053Z",
     "shell.execute_reply.started": "2024-11-12T15:24:36.459370Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: \"I will use Python to calculate the sum of the first 14 odd prime numbers.\\n\\n\"\n",
      "\n",
      "-----\n",
      "executable_code {\n",
      "  language: PYTHON\n",
      "  code: \"\\nimport sympy\\n\\nprimes = [x for x in sympy.primerange(1, 100) if x % 2 != 0]\\nprint(f\\'{primes[:14]=}\\')\\nprint(f\\'sum(primes[:14]) = {sum(primes[:14])}\\')\\n\"\n",
      "}\n",
      "\n",
      "-----\n",
      "code_execution_result {\n",
      "  outcome: OUTCOME_OK\n",
      "  output: \"primes[:14]=[3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47]\\nsum(primes[:14]) = 326\\n\"\n",
      "}\n",
      "\n",
      "-----\n",
      "text: \"The sum of the first 14 odd prime numbers is 326. \\n\"\n",
      "\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "for part in response.candidates[0].content.parts:\n",
    "  print(part)\n",
    "  print(\"-----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-11-12T15:24:58.785205Z",
     "iopub.status.busy": "2024-11-12T15:24:58.784749Z",
     "iopub.status.idle": "2024-11-12T15:25:01.888627Z",
     "shell.execute_reply": "2024-11-12T15:25:01.887275Z",
     "shell.execute_reply.started": "2024-11-12T15:24:58.785162Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This file is a Bash/Zsh script designed to enhance your command line prompt by incorporating Git repository information. It's called `bash-git-prompt`, and it's a powerful tool for developers who want to see the status of their Git repositories directly in their terminal. \n",
       "\n",
       "Here's a breakdown of its key features:\n",
       "\n",
       "**What it does:**\n",
       "\n",
       "* **Displays Git status:** The script dynamically updates your prompt to show the current branch, whether it's ahead or behind its remote counterpart, and any staged, unstaged, or untracked changes.\n",
       "* **Customizable:**  You can configure the script to display different information, change the colors, and choose from pre-defined themes, or even create your own.\n",
       "* **Virtual environment integration:** It integrates with virtual environments (like virtualenv, conda, and Node.js) to show which environment is active in your prompt.\n",
       "* **Works with both Bash and Zsh:**  The script ensures compatibility across these popular shells.\n",
       "\n",
       "**Why you would use it:**\n",
       "\n",
       "* **Improved workflow:** It provides a clear overview of your Git repository status without having to constantly run `git status`.\n",
       "* **Efficiency:**  It saves you time by eliminating the need to navigate to the terminal and run commands to check Git status.\n",
       "* **Customization:** It allows you to personalize your prompt to suit your preferences and workflows.\n",
       "* **Enhanced development environment:** By integrating with virtual environments, it provides a comprehensive view of your project's context within the terminal.\n",
       "\n",
       "**How to use it:**\n",
       "\n",
       "1. **Download and install:**  You can usually find the script on GitHub or other package repositories.\n",
       "2. **Source the script:** Add the following line to your Bash or Zsh configuration file (e.g., `.bashrc`, `.zshrc`):\n",
       "   ```bash\n",
       "   source /path/to/bash-git-prompt.sh\n",
       "   ```\n",
       "3. **Run the setup:**  The script likely has a setup function (e.g., `gp_install_prompt`) that you need to call to activate its features. \n",
       "\n",
       "This script can be a valuable addition to your development toolkit, enhancing your terminal experience and streamlining your Git workflows.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_contents = !curl https://raw.githubusercontent.com/magicmonty/bash-git-prompt/refs/heads/master/gitprompt.sh\n",
    "\n",
    "explain_prompt = f\"\"\"\n",
    "Please explain what this file does at a very high level. What is it, and why would I use it?\n",
    "\n",
    "```\n",
    "{file_contents}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "\n",
    "response = model.generate_content(explain_prompt, request_options=retry_policy)\n",
    "Markdown(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30786,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
